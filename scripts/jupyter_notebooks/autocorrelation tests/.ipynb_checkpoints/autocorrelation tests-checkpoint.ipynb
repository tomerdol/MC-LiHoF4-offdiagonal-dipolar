{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_correction(arr, iter):\n",
    "    binned_arr=np.copy(arr)\n",
    "    error_est_arr = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        M=binned_arr.size\n",
    "        error_est_arr[i]=math.sqrt(np.sum((binned_arr-np.mean(binned_arr))**2)/(M*(M-1)))\n",
    "        binned_arr=get_binned_array(binned_arr)\n",
    "    return error_est_arr\n",
    "  \n",
    "def get_binned_array(arr):\n",
    "    if arr.size%2==0:\n",
    "        return 0.5*(arr[::2]+arr[1::2])\n",
    "    else:\n",
    "        return 0.5*(arr[:-1:2]+arr[1::2])\n",
    "\n",
    "def calc_error_correction2(arr, bin_sizes):\n",
    "    print(len(bin_sizes))\n",
    "    error_est_arr = np.zeros(len(bin_sizes))\n",
    "    for i, bin_size in enumerate(bin_sizes):\n",
    "        binned_arr=get_binned_array2(arr, bin_size)\n",
    "        M=binned_arr.size\n",
    "        error_est_arr[i]=math.sqrt(np.sum((binned_arr-np.mean(binned_arr))**2)/(M*(M-1)))\n",
    "    return error_est_arr\n",
    "\n",
    "def get_binned_array2(arr, bin_size):\n",
    "    bin_num=arr.size//bin_size\n",
    "    binned_arr=np.zeros(bin_num)\n",
    "    for i in range(bin_num):\n",
    "        binned_arr[i]=np.mean(arr[i*bin_size:(i+1)*bin_size])\n",
    "    return binned_arr\n",
    "\n",
    "def auto_corr_fast(M):   \n",
    "#   The autocorrelation has to be truncated at some point so there are enough\n",
    "#   data points constructing each lag. Let kappa be the cutoff\n",
    "    M = M - np.mean(M)\n",
    "    N = len(M)\n",
    "    fvi = np.fft.fft(M, n=2*N)\n",
    "#   G is the autocorrelation curve\n",
    "    G = np.real( np.fft.ifft( fvi * np.conjugate(fvi) )[:N] )\n",
    "    G /= N - np.arange(N); G /= G[0]\n",
    "    G = G[:kappa]\n",
    "    return G\n",
    "\n",
    "def auto_corr_faster(M):\n",
    "#   The autocorrelation has to be truncated at some point so there are enough\n",
    "#   data points constructing each lag. Let kappa be the cutoff\n",
    "    M = M - np.mean(M)\n",
    "    M = M[:kappa]\n",
    "    N = len(M)\n",
    "    fvi = np.fft.fft(M)\n",
    "#   G is the autocorrelation curve\n",
    "    G = (np.abs(fvi)**2)[0]\n",
    "    #print((np.abs(fvi)**2))\n",
    "    G /= np.var(M)\n",
    "    return G\n",
    "\n",
    "def calc_tau(M):\n",
    "#   autocorr is the UNintegrated autocorrelation curve\n",
    "    autocorr = auto_corr2(M)    \n",
    "#   tau = 1 + 2*sum(G)\n",
    "    return 1 + 2*np.sum(autocorr), autocorr\n",
    "\n",
    "def auto_corr(M,t):\n",
    "    t_max=len(M)\n",
    "    return np.sum(M[:t_max-t]*M[t:])/(t_max-t) - \\\n",
    "           (np.sum(M[:t_max-t])/(t_max-t))*np.sum(M[t:])/(t_max-t)\n",
    "    \n",
    "def corr_func_katz(M,t):\n",
    "    #return (np.mean(M[:-t]*M[t:]) - np.mean(M[:-t])*np.mean(M[t:]))/(np.var(M))\n",
    "    return (np.mean(M[:-t]*M[t:]) - np.mean(M)*np.mean(M))/(np.var(M))\n",
    "\n",
    "def plot_corr_func(M):\n",
    "    auto_corr_func = np.zeros(int(M.size/10))\n",
    "    for t in range(int(M.size/10)):\n",
    "        print(t)\n",
    "        auto_corr_func[t]=corr_func_katz(M,t)\n",
    "    plt.plot(auto_corr_func)\n",
    "    plt.show()\n",
    "    \n",
    "def calc_tau2(M):\n",
    "    summation=0\n",
    "    for t in range(0,kappa-1):\n",
    "        summation+=auto_corr(M,t)\n",
    "    return summation/np.var(M)\n",
    "\n",
    "def auto_corr_wiki(M):\n",
    "    n=M.size\n",
    "    auto_corr=np.zeros(n)\n",
    "    mu=np.mean(M)\n",
    "    sigma2=np.var(M)\n",
    "    for k in range(n):\n",
    "        auto_corr[k] = np.sum((M[:n-k]-mu)*(M[k:]-mu)) / sigma2 / (n-k)\n",
    "        print(str(k)+' '+str(auto_corr[k]))\n",
    "    return auto_corr\n",
    "\n",
    "def auto_corr2(M):\n",
    "#   The autocorrelation has to be truncated at some point so there are enough\n",
    "#   data points constructing each lag. Let kappa be the cutoff\n",
    "    auto_corr = np.zeros(kappa-1)\n",
    "    mu = np.mean(M)\n",
    "    for s in range(0,kappa-1):\n",
    "        auto_corr[s] = np.mean( (M[:-s]-mu) * (M[s:]-mu) ) / np.var(M)\n",
    "    return auto_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=5\n",
    "h=0.0\n",
    "h_ex=0.0\n",
    "mech='false'\n",
    "obs='Magnetization'\n",
    "folderName='correlation_test'\n",
    "x=1.0\n",
    "#T=1.46976365365745\n",
    "#T=1.4681055099081104\n",
    "T=1.4998575151938878\n",
    "#T=1.4998575151938878\n",
    "#T=1.7638839354959759\n",
    "fname='C:\\\\path\\\\to\\\\project\\\\project6\\\\analysis\\\\'+folderName+'\\\\table_'+str(L)+'_'+str(L)+'_'+str(x)+'_'+str(h)+'_'+str(h_ex)+'_'+str(T)+'_'+mech+'_t.txt'\n",
    "y = pd.read_csv(fname, sep=\"\\t\", error_bad_lines=False, index_col='index', comment='#')\n",
    "print('done reading file!')\n",
    "arr=np.abs(y[obs].values)\n",
    "arr=arr[-int(arr.size/2):]\n",
    "#print(arr.size)\n",
    "\n",
    "#auto_corr_curve=auto_corr_wiki(arr)\n",
    "#plt.plot(auto_corr_curve)\n",
    "#plt.show()\n",
    "\n",
    "#plt.acorr(arr,usevlines=False)\n",
    "#plt.show()\n",
    "#autocorrelation_plot(arr)\n",
    "#plt.show()\n",
    "#print(arr.size)\n",
    "#plot_corr_func(arr)\n",
    "\n",
    "#auto_corr_func = np.zeros(3000)\n",
    "'''\n",
    "for t in range(3000):\n",
    "    auto_corr_func[t]=auto_corr(arr,t)\n",
    "plt.plot(auto_corr_func)\n",
    "plt.show()\n",
    "\n",
    "tau=0\n",
    "while (abs(5*tau-kappa)>1):\n",
    "    tau, curve = calc_tau(arr)\n",
    "    print((tau, 5*tau, kappa))\n",
    "    if (5*tau>kappa):\n",
    "        kappa+=1\n",
    "    else:\n",
    "        kappa-=1\n",
    "\n",
    "print('tau=%s'%(tau))\n",
    "'''\n",
    "#plt.plot(curve)\n",
    "#plt.show()\n",
    "#plt.plot(np.abs(arr),'.')\n",
    "#-1+int(0.5*math.log2(arr.size))\n",
    "#2+int(math.log2(arr.size/10.0))\n",
    "\n",
    "#last_level=8+int(0.5*math.log2(arr.size))\n",
    "#print('number of bins: %s'%int(arr.size*0.5**last_level))\n",
    "#error_corrections=calc_error_correction(arr,last_level)\n",
    "#iter_list=[1,2,4,8,16,32,64,128,160,192,224,256,320,384,448,512,1024]\n",
    "#error_corrections=calc_error_correction2(arr,iter_list)\n",
    "\n",
    "\n",
    "#plot binning analysis:\n",
    "Ns=[2,4,8,16,32,48]\n",
    "last_levels={2:12,4:12,8:12\n",
    "             ,16:9,32:9,48:8}\n",
    "for N in Ns:\n",
    "    fname='2dising_%s.txt'%N\n",
    "    y = pd.read_csv(fname, sep=\"\\t\", error_bad_lines=False, index_col='step', comment='#')\n",
    "    arr=np.abs(y[obs].values)\n",
    "    last_level=4+int(0.5*math.log2(arr.size))\n",
    "    last_level=int(math.log2(arr.size/N))\n",
    "    last_level=last_levels[N]\n",
    "    error_corrections=calc_error_correction(arr,last_level)\n",
    "    plt.plot(error_corrections,'-',label='N=%s'%N)\n",
    "\n",
    "# Ls=[2,3,4,5,6,7,8]\n",
    "# T=1.4998575151938878\n",
    "# for l in Ls:\n",
    "#     y = pd.read_csv(fname, sep=\"\\t\", error_bad_lines=False, index_col='index', comment='#')\n",
    "#     arr=np.abs(y[obs].values)\n",
    "#     arr=arr[-int(arr.size/2):]\n",
    "#     last_level=int(math.log2(arr.size/32))\n",
    "#     error_corrections=calc_error_correction(arr,last_level)\n",
    "#     plt.plot(error_corrections,'-',label='L=%s'%l)\n",
    "    \n",
    "    \n",
    "# last_level=int(math.log2(arr.size/32))\n",
    "\n",
    "# error_corrections=calc_error_correction(arr,last_level)\n",
    "# plt.plot(error_corrections,'-')\n",
    "\n",
    "# print('tau:')\n",
    "# print(0.5*((error_corrections/error_corrections[0])**2 - 1))\n",
    "#plt.plot(error_corrections,'.')\n",
    "#plt.gca().set_xscale('log',basex=2)\n",
    "plt.title('%s Binned sd vs. Binning level'%obs)\n",
    "#plt.title('%s Binned sd vs. bin size '%obs)\n",
    "plt.xlabel(r'Binning level $l$')\n",
    "#plt.xlabel('bin size')\n",
    "plt.ylabel(r'%s $\\Delta^{(l)}$'%obs)\n",
    "plt.legend()\n",
    "#plt.grid(b=True, which='both', axis='y')\n",
    "plt.margins(0)\n",
    "\n",
    "\n",
    "#plt.show.hist(arr,bins=50)\n",
    "plt.savefig('..\\\\figures\\\\binning_fig_new_seed2.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
